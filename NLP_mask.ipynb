{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Masked Language Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "twitter_data = load_dataset(\"cayjobla/twitter-sentiment-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet_id': 1753253621,\n",
       " 'sentiment': 8,\n",
       " 'content': '@aminorjourney - We owe you a LOT.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cayjobla/distilbert-base-uncased-finetuned-twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 12:04:23.814551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 12:04:23.999854: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-25 12:04:24.856527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-25 12:04:24.856617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-25 12:04:24.856628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] @ aminorjourney - we owe you a lot. [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = tokenizer(twitter_data[\"train\"][0][\"content\"])['input_ids']\n",
    "tokenizer.decode(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8fe213f9284e2e86bb60f83379c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb841cc15b1e457f8fdf78541ae15bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 32000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"content\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = twitter_data.map(\n",
    "    # Remove sentiment classification since our task is different\n",
    "    tokenize_function, batched=True, remove_columns=[\"tweet_id\", \"content\", \"sentiment\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 11'\n",
      "'>>> Review 1 length: 8'\n",
      "'>>> Review 2 length: 16'\n",
      "'>>> Review 3 length: 16'\n",
      "'>>> Review 4 length: 36'\n",
      "'>>> Review 5 length: 6'\n",
      "'>>> Review 6 length: 7'\n",
      "'>>> Review 7 length: 22'\n",
      "'>>> Review 8 length: 24'\n",
      "'>>> Review 9 length: 24'\n"
     ]
    }
   ],
   "source": [
    "# Get sample lengths\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:10]\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 170'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 42'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples, chunk_size=128):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0d5de3975047928cf962f1c34f91fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f491494f4074bd9a12acc09181e3515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 5219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1342\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] @ aminorjourney - we owe you a lot. [SEP] [CLS] chilling feeling really nice.. [SEP] [CLS] i'm soooo sleepy but i'm not a home just yet [SEP] [CLS] @ sarahsss i wish i had friends i could spend the night with [SEP] [CLS] @ johncmayer ur really the sweetest person ever! thanks for making everyone's dreams come true.. ( p. s ) my dream is for u 2twitter me back x [SEP] [CLS] juss boredd,! [SEP] [CLS] @ novawildstar damn right! [SEP] [CLS] @ marjicurran1 looking forward to your gig in ireland!!! see ya there! [SEP] [CLS] home from ghosts of girlfriends\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] @ aminorjourney - we owe you a lot. [SEP] [CLS] chilling feeling really nice.. [SEP] [CLS] i'm soooo sleepy but i'm not a [MASK] just yet [SEP] [CLS] @ sarahsss i wish i had friends i [MASK] spend the night with [SEP] [CLS] @ johncmayer ur [MASK] the sweetest person ever! thanks for making everyone's dreams come true [MASK]. ( p. [MASK] ) my dream is for [MASK] 2twitter me back x [SEP] [CLS] juss boredd,! [SEP] [CLS] @ novawildstar damn right! [SEP] [CLS] @ marjic [MASK]an1 looking forward to your gig in ireland!!! see ya there! [SEP] [CLS] home from ghosts of [MASK]'\n",
      "\n",
      "'>>> past with [MASK] lovely lud [MASK] not exactly high [MASK], but a good date movie! [SEP] [CLS] @ passed u r \" happiness pollinator \" 4 shizzle. good friend you are.. [MASK] am smiling! [SEP] [CLS] @ bam _ hall is that why you aint answer my call? [MASK] we were homies!? [SEP] [CLS] @ macquid no! strange, each time we [MASK] i [MASK] like i have come home and yet i have no spanish [MASK] that i know of. [MASK].. [SEP] [CLS] @ arielm27 [MASK] happy birthday [SEP] [CLS] i was on time [MASK] shifts in a row. [MASK] i woke up [MASK] 840 [MASK] [MASK]'s at 9'\n",
      "\n",
      "'>>> . five minutes late, ugh! - _ [MASK] [SEP] [CLS] @ hoptonhousebnb now theres a thought.... do [MASK] still get goldfish at the fair?? not seen them for years [SEP] [CLS] @ chuckdarw1n whew! thassa relief [SEP] [CLS] [MASK] were taking flight to [MASK] out tonight but they're now sinking in the hudson since i have to work in [MASK] morning. stupid bird strike. [SEP] [CLS] wrest [MASK]fest [MASK] fun! now tune into www. latalkradio [MASK] com from 2 - 3 sunday and chat [MASK] me [SEP] [CLS] stupid [MASK] weather ruined my [MASK] [MASK] [SEP] [CLS] had a good time at flap - [MASK] [MASK] taco'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 12:04:42.955555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-25 12:04:42.955590: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-25 12:04:42.956341: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Token based collator\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "# Word-based collator\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        word2idxs = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for token_idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                word2idxs[current_word_index].append(token_idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, len(word2idxs))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in word2idxs[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return tf_default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] @ aminorjourney - we owe you a lot [MASK] [SEP] [CLS] chilling feeling really nice. [MASK] [SEP] [CLS] [MASK]'m [MASK] sleepy [MASK] i'm not a home just yet [SEP] [CLS] @ sarahsss [MASK] [MASK] i [MASK] friends i could spend the night with [SEP] [CLS] [MASK] [MASK] [MASK] really the sweetest person ever [MASK] thanks for making [MASK] [MASK] s dreams come true.. ( p. [MASK] ) my dream is for u 2twitter me back x [SEP] [CLS] juss boredd [MASK]! [SEP] [CLS] @ novawildstar damn [MASK]! [SEP] [CLS] @ [MASK] [MASK] [MASK] [MASK] [MASK] looking forward to your gig in ireland!!! see ya [MASK]! [SEP] [CLS] home [MASK] [MASK] of [MASK]'\n",
      "\n",
      "'>>> past with my lovely luddite not [MASK] high theatre, but [MASK] good date movie [MASK] [SEP] [CLS] [MASK] rosehwang u r \" happiness pollinator \" [MASK] shizzle [MASK] good friend you are.. [MASK] am [MASK]! [SEP] [CLS] @ bam _ hall [MASK] [MASK] why you aint answer my call [MASK] thought we [MASK] homies! [MASK] [SEP] [CLS] [MASK] macquid no! strange, each time we return i feel like i have come home and yet i have no spanish [MASK] that i know of [MASK]... [SEP] [CLS] @ arielm27 happy [MASK] birthday [SEP] [CLS] i [MASK] on time three shifts in a row. but i woke [MASK] [MASK] [MASK] [MASK] [MASK]'[MASK] at 9'\n",
      "\n",
      "'>>> . five minutes late, ugh! - [MASK] - [SEP] [CLS] [MASK] hoptonhousebnb now [MASK] a thought.... do you still get goldfish [MASK] the [MASK] [MASK]? not seen [MASK] [MASK] [MASK] [SEP] [CLS] @ [MASK] [MASK] [MASK] whew [MASK] thassa relief [SEP] [CLS] plans were taking flight to go [MASK] tonight but they're now sinking [MASK] [MASK] hudson [MASK] i have to work [MASK] he morning. stupid [MASK] strike. [SEP] [CLS] wrestlefest was [MASK]! now tune into www. latalkradio. com from 2 [MASK] 3 sunday and chat [MASK] me [SEP] [CLS] [MASK] [MASK] weather ruined [MASK] [MASK] off [SEP] [CLS] had a [MASK] time at flap - a - taco'\n"
     ]
    }
   ],
   "source": [
    "# Word-based collator\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(3)]\n",
    "\n",
    "for chunk in whole_word_masking_data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " vocab_transform (Dense)     multiple                  590592    \n",
      "                                                                 \n",
      " vocab_layer_norm (LayerNorm  multiple                 1536      \n",
      " alization)                                                      \n",
      "                                                                 \n",
      " vocab_projector (TFDistilBe  multiple                 23866170  \n",
      " rtLMHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,985,530\n",
      "Trainable params: 66,985,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = lm_datasets.remove_columns([\"word_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-cayjobla/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_00,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# callback = PushToHubCallback(\n",
    "#     output_dir=f\"{model_name}-finetuned-twitter-mask\", tokenizer=tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 68s 6s/step - loss: 14.3800\n",
      "Perplexity: 1758471.09\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 673s 17s/step - loss: 10.6120 - val_loss: 8.6415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f313c341490>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, )#callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 63s 6s/step - loss: 8.6828\n",
      "Perplexity: 5900.57\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-twitter-mask\"\n",
    "tokenizer.save_pretrained(model_name)\n",
    "model.save_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0f70248f2e4720b33c38c4007e5ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForMaskedLM.\n",
      "\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at cayjobla/distilbert-base-uncased-finetuned-twitter.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916024d0c3843b08af5a9b6279c2143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390aeb27d65e4f79b7440c951d9e369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/205k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4859d75c1284484e9a31148cc34df02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/677k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"cayjobla/distilbert-base-uncased-finetuned-twitter-mask\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i went to the pain\n",
      ">>> i went to thepcom\n",
      ">>> i went to the prod\n",
      ">>> i went to theffee\n",
      ">>> i went to the fav\n"
     ]
    }
   ],
   "source": [
    "text = \"I went to the [MASK]\"\n",
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
